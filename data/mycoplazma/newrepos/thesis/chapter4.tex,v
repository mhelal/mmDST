head     1.1;
branch   1.1.1;
access   ;
symbols  r1:1.1.1.1 mhelal:1.1.1;
locks    ; strict;
comment  @% @;


1.1
date     2008.05.06.04.03.23;  author mhelal;  state Exp;
branches 1.1.1.1;
next     ;

1.1.1.1
date     2008.05.06.04.03.23;  author mhelal;  state Exp;
branches ;
next     ;


desc
@@



1.1
log
@Initial revision
@
text
@\chapter{Peer to Peer Solution}
In this chapter  we introduce the ideas behind the further developed distributed Peer-to-Peer proposed solution. Having started with a simple partitioning method as mentioned in section 3, experiments revealed the characteristics of the true parallelism in this model. It was found that not all ($2^{k}-1$) needed neighbors will be computed in one wave of computation, and then passed in the next wave to the current cell to be computed. The process computing a current cell score blocks waiting until all lower neighbors are computed (locally or remotely), and if remotely, the needed cell scores passed on to the current processor (by MPI sending/receiving operation) on \textit{k} (the dimensionality) waves of computations at most. The true parallelism were found to be dividing the cells into groups of equal distance from the origin, to be computed in one wave of true parallel computation. A later distance can be subsequently computed, having received the previous dependencies from up to \textit{k} previous waves, and so forth. The distance from the origin is measured by the summation of the indices in all dimensions (each first divided by the partitioning size \textit{S}), compared to the origin (zero in all dimensions). 

\section{P2P Partitioning}

Updating the partitioning method mentioned above to perform in P2P revealed the following characteristics of the parallelism. The waves are defined here as composed of partitions of equal distance from the origin, not of individual cells. Assuming there are \textit{t} waves, where \textit{$(w_{i})$} is the $(i^{th})$ wave where $0 \leq i < t$. In each $(w_{i})$, there will be $p_i$ partitions, divided equally in the cluster size \textit{V}, with the first $\frac{p_i}{V}$ (i.e. 0 .. $\frac{p_i}{V} - 1$) assigned to processor 0, the second $\frac{p_i}{V}$ (i.e. $\frac{p_i}{V}$ .. ($2 \times \frac{p_i}{V}) - 1$) parts going to processor 1, and so forth. The last remaining partitions in the wave are assigned to the last processor in the cluster. So, $p_{ij}$ is the $j^{th}$ partition in wave \textit{i}, where $0 \leq j < p_i$. Another way to reference a partition, is p$_{im}$, the set of all partitions in wave \textit{i} assigned to processor \textit{m}, where $0 \leq m < V$. Also, p$_m$ refers to all partitions in all waves assigned to processor \textit{m}.

Each partition has a number of cells to be computed internally, and higher border cells to be sent. The balance between internal computation and communication is based on the partitioning size \textit{S}. The cells to be computed are $S^k$, where \textit{k} is the dimensionality or the number of sequences in the input data. This includes the edge\textquoteright s lower indexed cells of the partitions that might be only initialized (these are the lower edges in the whole tensor), or available locally in other partitions in the same processor, or to be received in communication from remote partitions in other processors. Cell values received from another processor are buffered in the OCin buffer (Overlapping Cells - incoming). High border overlapping cells to be communicated in each partition are buffered in OCout (Overlapping Cells - outgoing). Both the OCin and OCout cells in each partition are a maximum of  $S^k - (S-1)^k$. The local lower edge cells, found in another partition in the local processor are retrieved from the local OCout buffer, rather than by searching all indices in the previous partitions.

\section{Wave Calculations}

Waves and their partitions are pre-processed, and then each processor fetches its partitions in the current computation wave, based on its index, the number of processors in the cluster, and the total number of partitions in the current computation wave. A recursive function assigns partitions to waves of computations, keeping adjacency for assignment over processors to minimize communication.

Partitions are scored in the current wave, then cell scores required by other partitions are sent to waiting processors in the next wave. We can think of waves as distance from the origin, with the origin at the initial partition of index vector of zeros $<0_0, 0_1, 0_2 ... 0_{k-1}>$, i.e. zero distance. The second wave consists of all partitions with indices with total distance 1 from the origin, (these are total scalar indices, each divided by \texttt{S-1}). This is the permutation of only one index of total 1, and the rest all zeros:\\
$(1_0, 0_1, 0_2 ... 0_{k-1})$\\
$(0_0, 1_1, 0_2 ... 0_{k-1})$\\
$(0_0, 0_1, 1_2 ... 0_{k-1})$\\
.\\
.\\
.\\
$(0_0, 0_1, 0_2 ... 1_{k-1})$\\

The third wave, is all indices of total distance 2, which is the permutation of distance vector of summation equals to 2. Such as all permutation of $(1_0, 1_1, 0_2, 0_3, .. 0_{k-1})$, and all permutations of $(2_0, 0_1, 0_2,  .. 0_{k-1})$, all in one wave. Then fourth wave is all permutations of indices of distance 3 from the origin. This will include permutations of $(1_0, 1_1, 1_2, 0_3, 0_4, .. 0_{k-1})$, $(2_0, 1_1, 0_2, 0_3, 0_4, .. 0_{k-1})$, and $(3_0, 0_1, 0_2, 0_3, ... 0_{k-1})$. and so forth.

Given these distance vectors, the actual index of the first cell in the partition (the partition index) is computed by multiplying by each scalar in the vector by $S-1$. A matrix of waves as the first dimension \textit{i}, and partition order as the second dimension \textit{j} is constructed containing the scalar starting index of the partition at wave \textit{i} and order \textit{j}.

This process (shown in the pseudocode below) continues until all lengths in all dimensions are covered. This creates a recursive counting algorithm that is called once before computation to fill in the 2 dimensional wave distribution structure (waves and their partitions\textquoteright starting indices). This wave distribution structure is checked by each processor to fetch its next partition iteratively until finished. The wave distribution is calculated by first looping to get each starting index, then looping through all permutations, checking if this permutation was used before to skip without adding it. The function keeps on calling the next index, until no more is returned.

The following two functions are used to calculate the wave distribution, starting from distance vector (dist) equals to the origin, $(0_{0}, 0_{1}, 0_{2}, ... 0_{k-1})$, where \textit{k} is the dimension, and wave number \textit{w = 0}:\\
\\
\textit{
\begin{small}\noindent\noindent getPartitionIndicesinWave \\
\indent	partsTotal = 0\\
\indent	more = true\\
\indent	do  \\
\indent\indent		/* return the distance vector, and more flag if there are\\
\indent\indent more indices at the same distance.*/\\
\indent\indent		getNextPartitionIndex (more, k, k, w, dist)\\
\indent\indent		/* Check if already added, multiply distance by\\
\indent\indent partition size and increment partsTotal */\\
\indent\indent		addPartitionIndex (partsTotal, dist, w) \\
\indent\indent		/* Copy the original distance before permutation */\\ 	
\indent\indent		dist-orig = dist\\
\indent\indent		pmore = 1\\
\indent\indent		/* include permutations of equal distance in the wave*/\\
\indent\indent		while (pmore == 1) \\ 
\indent\indent\indent			permute(k, dist, pmore)	\\
\indent\indent\indent			addPartitionIndex (partsTotal, dist, w)\\
\indent\indent		end while\\
\indent\indent		// Return the original distance before permutation\\
\indent\indent		dist = dist-orig\\
\indent	while (more == 1)\\
End Function\\
\end{small}
}

The next recursive function starts the distance from the origin by assigning the first dimension in the distance vector to $\frac{w}{k}$, where \textit{w} is the current wave number and \textit{k} is the dimensionality, and \textit{d=k}. Then recursively, the function is called with decreasing dimensions \textit{d} and decreasing wave distance \textit{w}, to distribute the remaining current wave distance ($w - \frac{w-\mbox{start distance}}{d}$) on the remaining dimensions of \textit{d=d-1} on each call, so that the current multidimensional index (vector of k scalar) sum up to wave number \textit{w}. On each call, the function starts from the middle point again of the passed w and d, and so forth. After all distance vector indices are assigned, the function returns with one partition index to the calling function above to be permuted to include all valid partitions at the same distance wave. Then the initial starting middle point is decremented, and the remaining value is distributed over the k-1 scalars of the distance vector, and so forth. For example at wave 3 (distance 3 from the origin), the first partition will be the fair distribution of 3 over the dimensionality, which is (1$_{0}$, 1$_{1}$, 1$_{2}$, 0$_3$, ... 0$_{k-1}$), then, (1$_{0}$, 2$_{1}$, 0$_{2}$, ..., 0$_{k-1}$), then finally (3$_{0}$, 0$_{1}$, 0$_{2}$, ... 0$_{k-1}$) and all their permutations. This keeps happening until the starting point becomes zero, and no wave distance remains to distribute. This method guarantees that we always start from the perfect middle diagonal of the tensor. and makes it easy to skip edge partitions, to reduce search space.\\
\\
\textit{
\begin{small}
\noindent getNextPartitionIndex (more, d, k, w, dist) \\
/* \textbf{more:} is a flag which is true if the recursive function is not yet called to the
last time, which is returning more indices, and false otherwise\\
\textbf{d:} is the current dimension value, after reductions in recursive calls\\
\textbf{k:} the original dimensionality of the problem,  (i.e. Number of sequences)\\
\textbf{w:} is the current wave number (distance from origin), after reductions in
the recursive call as well\\
\textbf{dist:} input/output is a vector of k elements, current distance from the origin permutation, representing a partition index */\\ \\
\noindent if (more = true) \\
\indent	if (w $>$ 0)\\
\indent\indent	startDist = w/k\\
\indent	else \\
\indent\indent	startDist = 0\\
\indent	End If\\
\indent	dist[d-1] = startDist\\
\indent	for (i=d-2;i$\geq$0;i--)\\
\indent\indent		dist[i] = 0\\
\noindent else \\
\indent	startDist = dist[d-1]\\
\noindent End If\\
/* All Terminal Cases first, then the 2D terminal cases, then the rest N-D cases \\
 test for wave 0 for all dimensions is the same, all zeros*/\\
if (w = 0) \\
\indent	for (i=0;i$<$k;i++) \\
\indent\indent		dist[i] = 0\\
\indent	more = true\\
else if	(dist[d-1] == w) \\
/* test for last case where the starting Distance is already equal the wave number, \\
and the rest of dimensions' indices are zeros*/\\
\indent	for (i=d-2;i$\geq$0;i--) \\
\indent\indent		dist[i] = 0\\
\indent	more = true\\
else if ((dist[d-1] * d == w) and (dist[d-1] != dist[d-2])) \\
/* All Equal Indices, starting middle point, i,e, in 3D $<$2,2,0$>$ */ \\
\indent	for (i=d-2;i$\geq$0;i--)\\
\indent\indent		dist[i] = dist[d-1]\\
\indent	more = false\\
else if (d = 2)  \\
/* Cases for 2D*/\\
\indent	if ((dist[d-2] != w - startDist)) \\
\indent	/* Firs Case, where first dimension = starting middle Distance, and second dimension is not yet\\ assigned*/\\
\indent\indent		dist[d-2] = w - startDist\\
\indent\indent		more = false\\
\indent	else if (startDist+1 $<$ w) \\
\indent	/* Increase one and decrease the other*/\\
\indent\indent		startDist++;\\
\indent\indent		dist[d-1] = startDist;\\
\indent\indent		dist[d-2] = w - startDist;\\
\indent\indent		more = false\\
\indent	else if (startDist+1 = w) \\
\indent	/* no more and return, we should never reach this case, since it is covered in case 111*/\\
\indent\indent		dist[d-1] = waveNo\\
\indent\indent		dist[d-2] = 0\\
\indent\indent		more = true\\
\indent 	End if\\
\noindent else \\
/* more the 2D*/\\
\indent	if  ((( dist[d-1] * d = w) and (dist[d-1] = dist[d-2])) or ((dist[d-1] + dist[d-2] = w) and\\ (dist[d-1]+1 $<$ w)))  \\
\indent	/* next case after all are equal First and second dimension are equal w, \\
	i.e. all permutations are finished for the previous first dimension,  then increase \\
	the first dimension*/\\
\indent\indent		startDist++\\
\indent\indent		dist[d-1] = startDist\\
\indent\indent		for (i=d-2;i$\geq$0;i--)\\
\indent\indent\indent			dist[i] = 0\\
\indent\indent		if (w - startDist != 0) \\
\indent\indent\indent			more = true\\
\indent\indent\indent			getNextPIndex (more, d-1, k, w - startDist, v)\\
\indent\indent\indent			more = false\\
\indent\indent		else\\
\indent\indent\indent			more = true\\
\indent\indent		End if\\
\indent	else if (dist[d-1] + dist[d-2] != w) \\
\indent	/* First and second dimension are not yet equal w, i.e. there more permutations for the current\\ first dimension value*/\\
\indent\indent		dist[d-1] = startDist\\
\indent\indent		getNextPIndex (more, d-1, k, w - startDist, v)\\
\indent\indent		more = false	\\
\indent	else if (dist[d-1]+1 = w) \\
\indent	/* last case, where the final increase will be the w, and the rest need to be zeros*/\\
\indent\indent		dist[d-1] = w\\
\indent\indent		for (i=d-2;i$\geq$0;i--) \\
\indent\indent\indent			dist[i] = 0\\
\indent\indent		more = true\\
\indent	End if\\
\noindent End if\\
End Function
\end{small}
}
\\
\\

The total partitions in a data set of dimensionality \textit{k}, and of sequence \textit{i} length l$_i$ ($0 \leq i < k$), and given a partitioning size \textit{S}, is the product of total partitions created in each dimension, and can be calculated as follows:

\begin{equation}
\Pi_{i=0}^{k} p_{i} =  (l_{i} - 1) / (S - 1)
\end{equation}	

Also, we can calculate the amount of duplicates cells (overlapping cells between partitions that will be included in more than one partition, calculated in one partition and used by other(s), and either communicated over processors, or fetched from local OCout buffer) as follows:

\begin{equation}
\label{TotalDuplicates}
\sum_{i=0}^{k}
\left\{\begin{array}{l l}
C_{i}=p_{i} - 1    \quad\mbox{ if i = 0} \quad \\
C_{i} = C_{i-1} \times l_{i} + (Pi_{j=0}^{i} p_{j}) \times 2^{i-1}  \quad\mbox{ if i $>$ 0} \quad
\end{array}
\right\rbrace 
\end{equation}	

A simple example with symmetry is 4 sequences with lengths (8, 8, 8, 8), which will be (9, 9, 9, 9) after including the initial gap character, resulting in 6561 cells to be computed. Dividing by a partition size S = 3 in each dimension, we get:\\
$p_{0} =  \frac{(9 - 1)} {3-1} \equiv 4$\\
$p_{1} =  \frac{(9 - 1)} {3-1} \equiv 4$\\
$p_{2} =  \frac{(9 - 1)} {3-1} \equiv 4$\\
$p_{3} =  \frac{(9 - 1)} {3-1} \equiv 4$\\
$p =  4 \times 4 \times 4 \times 4 \equiv 256$\\
\\
\\The overlapping cells are:

$C_{0}=p_{0} - 1 \equiv 3$
$C_{1}=3 \times 9 + 4 \equiv 31$
$C_{2}=31 \times 9 + 16 \times 2 \equiv 311$
$C_{3}=311 \times 9 + 64 \times 4 \equiv 3055$

$C = 3 + 31 + 311 + 3055 \equiv 3400$

Generally this will scale as shown in table I, which lists the number of partitions that can be done in parallel in one wave.

\begin{table}
\begin{center}
\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|}
\hline
 &  \multicolumn{8}{}{} Waves&\\
\hline
k &	1&	2&	3&	4&	5&	6&	7&	8&	9\\
\hline
2&		1&	2&	3&	4&	5&	6&	7&	8&	9\\
\hline
3&		1&	3&	6&	10&	15&	21&	28&	36&	45\\
\hline
4&		1&	4&	10&	20&	35&	56&	84&	120&	165\\
\hline
5&		1&	5&	15&	35&	70&	126&	210&	330&	495\\
\hline
6&		1&	6&	21&	56&	126&	252&	462&	792&	1287\\
\hline
7&		1&	7&	28&	84&	210&	462&	924&	1716&	3003\\
\hline
8&		1&	8&	36&	120&	330&	792&	1716&	3432&	6435\\
\hline
9&		1&	9&	45&	165&	495&	1287&	3003&	6435&	12870\\
\hline
\end{tabular}
\caption{Partitions Parallelism (Total Partitions per wave) shown for several dimensionality in rows, and several waves in columns, where k is the dimensionality}
\end{center}
\end{table}

This partitions scalability is illustrated in figure I.\\

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 240 412]{PartitionsWavesGrowth.png}
 % PartitionsWavesGrowth.png: 240x412 pixel, 72dpi, 8.47x14.53 cm, bb=0 0 240 412
\end{center}
\caption{Total Partitions / Wave growth graph for several dimensionality}
\end{figure} 

For the previously mentioned example, there are 256 partitions that need to be computed in 13 waves. Table II shows the distribution of partitions over the waves, according to the scheme shown in table I.

\begin{table}
\begin{center}
\begin{tabular}{|r|r|r|r|}
\hline
k&    W&    P$_{w}$& 	$\sum_{i=1}^w Pi$ \\
\hline
4&       1&       1& 		1\\
\hline
4&       2&       4&		5\\
\hline
4&       3&       10&		15\\
\hline
4&       4&       20&		35\\
\hline
4&       5&       31&		66\\
\hline
4&       6&       40&		106\\
\hline
4&       7&       44&		150\\
\hline
4&       8&       40&		190\\
\hline
4&       9&       31&		221\\
\hline
4&       10&      20&		241\\
\hline
4&       11&      10&		251\\
\hline
4&       12&      4&		255\\
\hline
4&       12&      1&		256\\
\hline
\end{tabular}
\caption{Partitions Parallelism (Total Partitions per wave) shown for dimensionality 4, where k is the dimensionality, w is the wave number, P$_{w}$ is the total number of partitions in the current wave w, and $\sum_{i=1}^w Pi$ is the total number of partitions done so far from wave 1 until the current wave \textit{w}.}
\end{center}
\end{table}

Notice at wave number 5, a maximum of 35 partitions can be calculated in this wave in 4 dimensions as per table I, but only 31 are needed in this particular data set (table II). The same limitation occurs in waves 6 and 7, where the original scalability can reach up to 56 and 84 respectively. Then, in wave number 8, the partitions total per wave start to decay symmetrically. The actual total partitions per wave, and hence total waves, is based on the availability of indices not processed yet, so they can be less partitions per wave than the maximum shown in table I, hence creating more waves than shown the above scheme. 


The partition\textquoteright s initial cell index is retrieved based on the number of indices changed from one wave to another. The initial wave will always contain one partition with indices $(0_0, 0_1, 0_2, ..., 0_{k-1})$ to be scored by the first processor in the cluster. A 3D distribution of partitions per wave is illustrated in table III.

\begin{table}
\begin{center}
% use packages: array
\begin{tabular}{|p{0.25cm}|l|p{6cm}|}
\hline
W&    P$_{w}$& 	Parts Indices \\
\hline
0&	1&	(0, 0, 0)\\
\hline
1&	3&	(0, 0, 2)	(0, 2, 0)	(2, 0, 0)\\
\hline
2&	6&	(0, 2, 2)	(2, 0, 2)	(2, 2, 0)	(0, 0, 4)	(0, 4, 0)	(4, 0, 0)\\
\hline
3&	10&	(2, 2, 2)	(0, 2, 4)	(0, 4, 2)	(2, 0, 4)	(2, 4, 0)	(4, 0, 2)	(4, 2, 0)	(0, 0, 6)	(0, 6, 0)	(6, 0, 0)\\
\hline
4&	12&	(2, 2, 4)	(2, 4, 2)	(4, 2, 2)	(0, 4, 4)	(4, 0, 4)	(4, 4, 0)	(0, 2, 6)	(0, 6, 2)	(2, 0, 6)	(2, 6, 0)	(6, 0, 2)	(6, 2, 0)\\
\hline
5&	12&	(4, 2, 4)	(4, 4, 2)	(2, 4, 4)	(0, 6, 4)	(4, 0, 6)	(4, 6, 0)	(6, 0, 4)	(6, 4, 0)	(0, 4, 6)	(2, 2, 6)	(2, 6, 2)	(6, 2, 2)\\
\hline
6&	10&	(4, 4, 4)	(4, 2, 6)	(4, 6, 2)	(6, 2, 4)	(6, 4, 2)	(2, 4, 6)	(2, 6, 4)	(0, 6, 6)	(6, 0, 6)	(6, 6, 0)\\
\hline
7&	6&	(4, 4, 6)	(4, 6, 4)	(6, 4, 4)	(2, 6, 6)	(6, 2, 6)	(6, 6, 2)\\
\hline
8&	3&	(6, 4, 6)	(6, 6, 4)	(4, 6, 6)\\
\hline
9&	1&	(6, 6, 6)\\
\hline
\end{tabular}
\\
\caption{A 3D distribution of partitions per wave, showing the partitions indices (multiplied by the partition size S-1 where S=3). Every wave starts from the middle partitions available in the wave and going to the edges, all of equal distance from the origin (summation of all indices in all dimensions) }
\end{center}
\end{table}


\section{Processor Assignments and load balancing}

The dependency networks can be illustrated geometrically using figures 2-4 for 2D, 3D and 4D. The axes in the 2D example represent the indices of the cells to be scored. The vertices in the diagram are the cells (or grouped into partitions of a number of cells), and the diagonals show the number of cells (or partitions) that can be computed simultaneously.

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 220 157]{2DGeom.png}
 % 2DGeom.png: 220x157 pixel, 72dpi, 7.76x5.54 cm, bb=0 0 220 157
\end{center}
\caption{Geometric representation of 2D waves and partitions indices, shown on a 2D plane, where waves are the diagonals connecting the partitions indices}
\end{figure} 

In 3D and 4D, only the base is drawn. In 3D, the origin point in the middle of the three axes, will triple every 3 waves, in a third plane that is not shown in this illustration.

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 220 184]{3DGeom.png}
 % 3DGeom.png: 220x184 pixel, 72dpi, 7.76x6.49 cm, bb=0 0 220 184
\end{center}
\caption{Geometric representation of 3D waves and partitions indices, shown on a 3D base plane only. Axes are represented with lines having dashes at fixed intervals showing the distances from the origin (where the axes intersects), other lines show the waves connecting partitions (vertices) of equal distance from the origin. A third plane project from this plane, and triple every 3 waves.}
\end{figure} 

In the 4D another 2 planes will be projected, and grow more faces, as we go from one wave to another. 

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 220 184]{4DGeom.png}
 % 4DGeom.png: 220x184 pixel, 72dpi, 7.76x6.49 cm, bb=0 0 220 184
\end{center}
\caption{Geometric representation of 4D waves and partitions indices, shown on a 4D base plane only. Axes are shown with dashes at fixed intervals from the origin, and squares around the origin show the waves. Two planes project from this base, and quadruple every 4 waves}
\end{figure} 

Figures 2-4 show the waves of computations around the origin point and rotating the axes. They form regular polytopes (a higher dimensional analogue of regular polygons in 2D, polyhedra in 3D, and polychora in 4D, and so forth). This representation can direct the research into more geometric analysis of the index transformations that can lead to better optimized closed formula moves from one cell to its dependent cells. The MoA formula discussed in chapter 3 is to date the only one known to the authors that scales effectively with dimensions, wave number, partition index, and cell index.

The dependency network showing the connections between cells (or partition) in one wave with their dependent cells (or partitions) in the next wave is shown for a 2D example in figure 5.

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 220 190]{2DDepNetwork.png}
 % 2DDepNetwork.png: 220x190 pixel, 72dpi, 7.76x6.70 cm, bb=0 0 220 190
\end{center}
\caption{2D dependency network connecting partitions in one wave to their dependent partition(s) in the next wave. Dependencies required from up to k waves are passed from one wave to another. For instance, cell 5 needs cells 2 and 3 from wave 1, and cell 5 from wave 0. Cell 5 is sent to wave 1 first, then again considered overlapping cell in wave 1 and send to wave 2.}
\end{figure} 

The 3D dependency network is also straight forward to represent in figure 6.

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 220 161]{3DDepNetwork.png}
 % 3DDepNetwork.png: 220x161 pixel, 72dpi, 7.76x5.68 cm, bb=0 0 220 161
\end{center}
\caption{3D dependency network connecting partitions in one wave to their dependant partition(s) in the next wave. Growth of parallelism is obvious compared to the 2D example. Cell 15 needs cells 6, 7, and 8 from wave 2, and cells 2, 3, and 4 from wave 1, and cell 1 from wave 0. Cells from K waves (here 3) are sent from one wave to another until received to the dependent cell}
\end{figure} 

The 4D network is more difficult to visualize, but is plotted in figure 7 to demonstrate that symmetry is maintained as dimension increases.

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 220 97]{4DDepNetwork.png}
 % 4DDepNetwork.png: 220x97 pixel, 72dpi, 7.76x3.42 cm, bb=0 0 220 97
\end{center}
\caption{4D dependency network. Parallelism growth is obvious again, as well as symmetric connections.}
\end{figure} 

All nodes in the above networks can represent individual cells, or starting indices of partitions of size \textit{S} in the whole tensor.


A very simple and intuitive load balancing technique was found inherent in the conformal computing methods, based on neighborhood and adjacency of indices. In each wave, partitions are divided among processors: the first quotient of partitions (plus one partition of the remainder if any, shifting the starting quoteint for the following processors) is to be assigned to the first processor, the second quotient (plus one partition of the remainder if any) to the second processor and so forth. Given that the order of partitions in the wave are based on neighborhood as shown in the pseudocode above, neighborhood of partitions are maintained within the wave. Moreover, starting in each wave from processor zero will guarantee that adjacency is also kept across the waves as the index increases one step away from the origin. Partitions\textquoteright   indices assigned to each processor across the wave, remain the closest neighbors within and across the waves, reducing communication cost. The colors in the 2D and 3D dependency networks graphs (figures 5 and 6) show the clustering of partitions over three processors (white, light gray and dark gray). A hash function is used to map a processor id to a partition index, and is called everytime a dependency needs to identify a waiting processor.

\section{P2P Scoring and Dependency Communication}


The same scoring scheme used in the Master/Slave design, and detailed in the complexity analysis section below is used in the P2P design. However, dependency is packed to be communicated in between waves of computation, and done by each processor independently, and not by a master process. In the current implementation, the dependency relationships are computed by retrieving the higher indexed neighbors of the current cell, their partition index and the corresponding processor. If the neighbor partition is found in a different processor, it is added to the OCout (Overlapping Cells outgoing) buffer, to be sent after all the cells in the partition are scored. Then all OCout cells are packed to be sent in one buffered MPI communication per processor to reduce the TCP/IP overhead.

The dependency networks for 2D, 3D, and 4D examples (figures 5-7), show an obvious symmetric relationship between dimension, wave number, partition order in wave, and dependent partitions in the next wave. However a closed formula describing this relationship has not been derived yet. The MoA equation number (1) currently used to retrieve the higher indexed neighbors remains the only known scalable solution (dimensionality - wave number - and partition coordinates - wise). 

\section{Complexity Analysis}


The complexity of the proposed solution is divided into 2 sections: the computation and the communication. Both are dependant on the partitioning size S, and load balancing. As mentioned above, the computation complexity of the scoring of the l$^{th}$ cell (s$_{l}$), where 0 $\leq$ l $<$ S$^k$,  in one partition (p$_{ij}$), in one computational wave (w$_{i}$), in one processor m, is as follows:

\begin{itemize}
  \item Comparison of global index to retrieve the current cell position in the whole tensor $\equiv$ O(k)
  \item Process the current cell based on its position:
\begin{itemize}
	\item If initial cell, initialize $\equiv$ O(1)
	\item If local low border internal cell, do the following in sequence till found: 
	\begin{itemize}
		\item check the (OCout) buffer for the previous wave $ \equiv  O(p_{(i-1),m} \times (S^k - (S-1)^k)) \equiv O(S^k)$
		\item check received list (OCin) for the previous wave $\equiv O(p_{(i-1),m} \times (S^k - (S-1)^k)) \equiv O(S^k)$
		\item Block wait till received $\equiv$ O(1)
	\end{itemize}
	\item If not local low border, but internal cell, compute the score as follows: 
	\begin{itemize}
		\item get pair-wise scores for each pair of residues  $\equiv$ O(k$^{2}$)
		\item get lower neighbors\textquoteright  scores  $\equiv$ O(2$^{k}$ $-$ 1)
		\item compute temporary neighbor score $\equiv$ O(3 $\times$ 2$^{k}$ $-$ 1)
		\item assign maximum temporary score to current cell score   $\equiv$ O(1)
	\end{itemize}
\end{itemize}
  \item Dependency Analysis $\equiv O(p_{(i+1),m} \times (S^k - (S-1)^k)) \equiv O(S^k)$
\end{itemize}

The worst case complexity when scoring a cell up to the block wait step, is: $O(k) + O(S^k) + O(S^k) + O(1) + O(S^k) \equiv O(S^k)$\\
\\
After a cell is computed, a dependency analysis is conducted, by comparing the cell\textquoteright s index to that of all lower indexed border cells in the partitions assigned to the following wave. The cell\textquoteright s score is sent only if the processor computing the dependant partition in the next wave is different from the processor that scored the current cell. Then an array of OCout (outgoing overlapping cells) will be formed, with the cell index, score, dependant processor, partition index, and wave number information. 

The execution time in a distributed environment is estimated by equation number (4), as defined by \cite{Stone-77}:

\begin{equation}
dT = r \times Max(p_{m}) + (\frac{c}{2}) \times (M^2 â€“ \sum p_{m}^2)
\end{equation}

where dT is the distributed overall time \\
r is the execution time of a single task on a processor, which is O(S$^k$) for each cell in each partition, in each wave assigned to a processor m \\
c is the communication cost between the single task and other tasks on other processors, which is equivalent to  O(S$^k$) again\\
M is the total tasks, which is total partitions in all waves\\
p$_{m}$ is number of tasks (partitions) assigned to m$^{th}$ processor.\\
\\

The r and c are attributes of the hardware we are executing on. We can also calculate R, which is the total computation time for all partitions assigned to a processor, and C which is total communication done by a processor for all partitions assigned to it. R and C can be easily calculated based on the partition size, and the ratio $\frac{R}{C}$ is the granularity of the design. 

\section{Implementation Details and Optimization}

\subsection{Checkpointing and Restoration}

The program checkpoints the pre-processing data (partitions indices and distribution over waves) to a main file. Then a secondary file of system states is saved (current wave number, total partitions scored so far, current partition index being scored, and overlapping cells sent and received). Also, a separate file for each partition scores is kept. The system can then restore 2 waves prior to the last wave that was computed before a crash. It is easier to go 2 waves back to ensure that all needed communication will be processed again, rather than starting from where the crash happened and building an ad-hoc communication model to send and receive specific missing scores.

\subsection{Optimization}

The choice of partition size will decide how much computation will be done in each partition, and hence how much communication will be sent/received. To minimize the total execution time of the distributed solution as shown in equation 4, two constraints need to be met: first, each partition should contain as much computation as can be done simultaneously without blocking for dependencies; second, the partition size (S$^{k}$) should fit in the processor local cache, so that we don't end up with much memory context switches, and degrade the overall performance. Communication generally needs to be minimized as well. Two approaches were used to minimize the communication penalty; first, packing to reduce the TCP/IP overhead; second, clustering partitions on processors based on neighborhood, so that the needed dependencies can be found locally. 

Figures 8 and 9 show how the computation (total internally computed cells in all partitions) and communication (dependency communicated between processors, or searched in other local partitions), vary with partition size. Communication grows at a slower rate then computation: larger partitions will always result in more computation than communication. The graphs also show the variations over a range of dimensionalities.

\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 228 426]{CompGrowth.png}
 % CompGrowth.png: 228x426 pixel, 72dpi, 8.04x15.03 cm, bb=0 0 228 426
\end{center}
\caption{Total cells to be computed in each partition in relation to partition size and dimensionality.}
\end{figure} 


\begin{figure}
\begin{center}
 \includegraphics[bb=0 0 232 426]{CommGrowth.png}
 % CommGrowth.png: 232x426 pixel, 72dpi, 8.18x15.03 cm, bb=0 0 232 426
\end{center}
\caption{Total number of cell scores to be communicated or searched for in previous local partitions, in each partition, as a function of partition size and dimensionality.}
\end{figure} 

Equation 4 shows that r and c will affect the total distributed execution time: r here can be thought of as the computation of one partition shown in figure 8, and c is the amount of computation made in each partition, as shown in figure 9. Taking the constraints of the machine used (cache memory size, communication speed, and processing speed) and the constraints of the data set (dimensionality and shape) as parameters, an objective function for minimizing the total distributed execution time in equation 4 can be developed. The optimization procedure should decide the optimal partition size, and the optimal number of processors to use.

A number of potential optimization solutions are available. An open source simplex algorithm can easily be embedded to the program. Alternatively, statistical packages such as the R Project for Statistical Computing\cite{R-Website} or A Modeling Language for Mathematical Programming (AMPL)\cite{AMPL-Website} can be used. These packages include optimization solvers of different algorithms that need input files formalizing equation (4), the data set constrains, and the hardware constraints.


@


1.1.1.1
log
@Thesis Writing
@
text
@@
